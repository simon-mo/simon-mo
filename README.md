### ðŸ‘‹ I'm Simon.

Currently, I'm a PhD student at Berkeley Sky Computing Lab for machine learning system and cloud infrastructures. I am advised by [Prof. Joseph Gonzalez](https://people.eecs.berkeley.edu/~jegonzal/). 

My latest focus is building an end to end stack for LLM inference on your own infrastructure. This work includs
- [vLLM](https://github.com/vllm-project/vllm) runs LLM inference efficiently. 
- Conex builds, push, and pull containers fast.
- SkyATC orchestrate LLMs in multi-cloud and scaling them to zero. 

I previously work on *Model Serving System* @anyscale. 
- [Ray](https://github.com/ray-project/ray) takes your Python code and scale it to thousands of cores.
- [Ray Serve](https://docs.ray.io/en/latest/serve/index.html#rayserve) empowers data scientists to own their end-to-end inference APIs.

Before Anyscale, I was a undergraduate researcher @ucbrise.

Publications:
- Under submission: [Optimizing LLM Queries in Relational Workloads](https://arxiv.org/abs/2403.05821)
- NSDI 2024: [Cloudcast: High-Throughput, Cost-Aware Overlay Multicast in the Cloud](https://www.usenix.org/conference/nsdi24/presentation/wooders) plan the best network for cloud object store replications.
- VLDB 2024: [RALF: Accuracy-Aware Scheduling for Feature Store Maintenance
](https://vldb.org/pvldb/volumes/17/paper/RALF%3A%20Accuracy-Aware%20Scheduling%20for%20Feature%20Store%20Maintenance) proposes feature update in feature store can be a lot more efficient. 
- SoCC 2020: [InferLine: ML Inference Pipeline Composition Framework](https://arxiv.org/abs/1812.01776) studies how to optimize model serving pipelines.
- VLDB 2020: [Towards Scalable Dataframe Systems](http://www.vldb.org/pvldb/vol13/p2033-petersohn.pdf) formalizes Pandas DataFrame.
- SysML Workshop @ Neurips 2018: [The OoO VLIW JIT Compiler for GPU Inference](https://arxiv.org/abs/1901.10008) tries to multiplex many kernels on the same GPU.

Reach out to me: simon.mo at hey.com
