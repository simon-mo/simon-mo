### ðŸ‘‹ I'm Simon.

Currently, I'm a PhD student at Berkeley Sky Computing Lab for machine learning system and cloud infrastructures.

My latest focus is building an end to end stack for LLM inference on your own infrastructure. 

I previously work on *Model Serving System* @anyscale. 
- [Ray](https://github.com/ray-project/ray) takes your Python code and scale it to thousands of cores.
- [Ray Serve](https://docs.ray.io/en/latest/serve/index.html#rayserve) empowers data scientists to own their end-to-end inference APIs.

Before Anyscale, I was a student researcher @ucbrise:
- SoCC 2020: [InferLine: ML Inference Pipeline Composition Framework](https://arxiv.org/abs/1812.01776) studies how to optimize model serving pipelines.
- VLDB 2020: [Towards Scalable Dataframe Systems](http://www.vldb.org/pvldb/vol13/p2033-petersohn.pdf) formalizes Pandas DataFrame.
- [The OoO VLIW JIT Compiler for GPU Inference](https://arxiv.org/abs/1901.10008) tries to multiplex many kernels on the same GPU.

Reach out to me: simon.mo at hey.com
